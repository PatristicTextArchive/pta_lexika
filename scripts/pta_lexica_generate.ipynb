{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d69a9-631f-441b-aa6d-a3b7d24ceedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,csv,re,sys\n",
    "import unicodedata\n",
    "# convert to xml\n",
    "from dicttoxml import dicttoxml\n",
    "from xml.dom.minidom import parseString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d1762-c473-417a-9567-b466dcf8149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wordlemma_grc.json: word - lemma correspondences generated from files in pta_data \n",
    "data_file = open('/home/stockhausen/Dokumente/projekte/pta_lexika/wordlemma_grc.json')    \n",
    "lemma_data = json.load(data_file)\n",
    "# load LSJ and save to json\n",
    "with open('/home/stockhausen/Dokumente/projekte/pta_lexika/sources/LSJ-6.5.1.txt', encoding='utf-8-sig') as file:\n",
    "    fnames = ['w', 'd']\n",
    "    reader = csv.DictReader(file, delimiter=\"\\t\", fieldnames=fnames)\n",
    "    lsj = []\n",
    "    for line in reader:\n",
    "        lsj.append(line)\n",
    "with open('/home/stockhausen/Dokumente/projekte/pta_lexika/LSJ.json', 'w') as fout:\n",
    "    json.dump(lsj, fout, indent=4, ensure_ascii=False)  \n",
    "# load Pape and save to json\n",
    "csv.field_size_limit(500000)\n",
    "with open('/home/stockhausen/Dokumente/projekte/pta_lexika/sources/Pape-4.6b.txt', encoding='utf-8-sig') as file:\n",
    "    fnames = ['w', 'd']\n",
    "    reader = csv.DictReader(file, delimiter=\"\\t\", fieldnames=fnames)\n",
    "    pape = []\n",
    "    for line in reader:\n",
    "        pape.append(line)\n",
    "with open('/home/stockhausen/Dokumente/projekte/pta_lexika/Pape.json', 'w') as fout:\n",
    "    json.dump(pape, fout, indent=4, ensure_ascii=False)  \n",
    "# load TBESG - Translators Brief lexicon of Extended Strongs for Greek - STEPBible.org CC BY and save to json\n",
    "with open('/home/stockhausen/Dokumente/projekte/pta_lexika/sources/TBESG - Translators Brief lexicon of Extended Strongs for Greek - STEPBible.org CC BY.txt', encoding='utf-8-sig') as file:\n",
    "    fnames = ['s','t','w','g','m','d']\n",
    "    reader = csv.DictReader(file, delimiter=\"\\t\", fieldnames=fnames)\n",
    "    TBESG = []\n",
    "    # remove columns not needed for our purposes\n",
    "    for line in reader:\n",
    "        line.pop('s')\n",
    "        line.pop('t')\n",
    "        line.pop('g')\n",
    "        line.pop('m')\n",
    "        TBESG.append(line)\n",
    "with open('/home/stockhausen/Dokumente/projekte/pta_lexika/TBESG.json', 'w') as fout:\n",
    "    json.dump(TBESG, fout, indent=4, ensure_ascii=False)  \n",
    "# load Bailly 2020 and save to json\n",
    "with open('/home/stockhausen/Dokumente/projekte/pta_lexika/sources/Bailly2020.3a.txt', encoding='utf-8-sig') as file:\n",
    "    fnames = ['w', 'd']\n",
    "    reader = csv.DictReader(file, delimiter=\"\\t\", fieldnames=fnames)\n",
    "    Bailly = []\n",
    "    for line in reader:\n",
    "        Bailly.append(line)\n",
    "    Bailly = Bailly[10:] # remove header\n",
    "with open('/home/stockhausen/Dokumente/projekte/pta_lexika/Bailly2020.3a.json', 'w') as fout:\n",
    "    json.dump(Bailly, fout, indent=4, ensure_ascii=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6afe803-07f1-409b-9793-dea75505ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lexika to xml and write to files\n",
    "count = 0\n",
    "for lexikon in [lsj,pape,TBESG,Bailly]:\n",
    "    count = count+1\n",
    "    xml = dicttoxml(lexikon, attr_type=False)\n",
    "    xml2 = xml.decode(\"utf-8\")\n",
    "    s = re.sub(r'&gt;', r'>', xml2)\n",
    "    s = re.sub(r'&lt;', r'<', s)\n",
    "    s = re.sub(r'&quot;', r'\"', s)\n",
    "    s = re.sub(r'&apos;',r'\"',s)\n",
    "    filename = \"/home/stockhausen/Dokumente/projekte/pta_lexika/\"+str(count)+\".xml\"\n",
    "    with open(filename, 'w', encoding='utf-8-sig') as file_open:\n",
    "        file_open.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0bcd15-dbb2-47b9-8554-ac0f9da16902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemma-Liste aus wordlemma.json\n",
    "lemmas = []\n",
    "lemmas2 = []\n",
    "for entry in lemma_data:\n",
    "    lem = entry[\"Lemma\"]\n",
    "    lemmas.append(lem)\n",
    "for i in lemmas:\n",
    "    s = re.sub(r'[0-9]+', r'', str(i)) # remove numbers from lemma entries\n",
    "    s = s.split(\"|\") # split entries at |\n",
    "    lemmas2.append(s)\n",
    "lemma_list = [item for sublist in lemmas2 for item in sublist]\n",
    "lemma_list = sorted(list(set(lemma_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f98f4c-8fa2-4d9c-a319-434796c68775",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lemma_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac53cf-ce14-4a78-846a-ef2a2de8a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingBar(count,total,size):\n",
    "    percent = float(count)/float(total)*100\n",
    "    sys.stdout.write(\"\\r\" +str(int(count)).rjust(3,'0')+\"/\"+str(int(total)).rjust(3,'0') + ' [' + '='*int(percent/10)*size + ' '*(10-int(percent/10))*size + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c5062-6799-4042-a596-ace30f282d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = []\n",
    "count = 0\n",
    "for entry in lemma_list:\n",
    "    count = count+1\n",
    "    loadingBar(count,len(lemma_list),2)\n",
    "    l_entry = {}\n",
    "    lookup = unicodedata.normalize(\"NFC\", entry)\n",
    "    l_entry[\"lemma\"] = lookup\n",
    "    l_entry[\"grc_eng\"] = [element[\"d\"] for element in lsj if unicodedata.normalize(\"NFC\", element['w']).lower() == lookup.lower()]\n",
    "    l_entry[\"grc_eng2\"] = [element[\"Meaning\"] for element in TBESG if unicodedata.normalize(\"NFC\", element['Greek']).lower() == lookup.lower()]\n",
    "    l_entry[\"grc_deu\"] = [element[\"d\"] for element in pape if unicodedata.normalize(\"NFC\", element['w']).lower() == lookup.lower()]\n",
    "    l_entry[\"grc_fra\"] = [element[\"d\"] for element in Bailly if unicodedata.normalize(\"NFC\", element['w']).lower() == lookup.lower()]\n",
    "    lexicon.append(l_entry)\n",
    "liste = [i for n, i in enumerate(lexicon) if i not in lexicon[n + 1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ee28e-82b2-4544-91c7-b6e046b1b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/stockhausen/Dokumente/projekte/pta_lexika/pta_lexicon_grc.json', 'w') as fout:\n",
    "    json.dump(lexicon, fout, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b684e-0398-4b5c-b8fb-f894af1cb3f4",
   "metadata": {},
   "source": [
    "### convert json to xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9ebe1-9bd3-4756-953e-2ac23185319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open('/home/stockhausen/Dokumente/projekte/pta_lexika/wordlemma_grc.json')    \n",
    "lemma_data = json.load(data_file)\n",
    "# split lemma and morphology to subentries\n",
    "lemma_data2 = []\n",
    "for entry in lemma_data:\n",
    "    lemmata = {}\n",
    "    lemmata[\"Word\"] = entry[\"Word\"]\n",
    "    lemmata[\"Lemma\"] = entry[\"Lemma\"].split(\"|\")    \n",
    "    lemmata[\"Morphology\"] = entry[\"Morphology\"].split(\"|\")\n",
    "    lemma_data2.append(lemmata)\n",
    "# convert json2 xml\n",
    "xml = dicttoxml(lemma_data2, attr_type=False)\n",
    "dom = parseString(xml)\n",
    "with open(\"/home/stockhausen/Dokumente/projekte/pta_lexika/wordlemma_grc.xml\", 'w') as file_open:\n",
    "    file_open.write(dom.toprettyxml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56938b-a8da-4f6e-85cd-411754e70f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file2 = open('/home/stockhausen/Dokumente/projekte/pta_lexika/pta_lexicon_grc.json')    \n",
    "lexicon_data = json.load(data_file2)\n",
    "xml2 = dicttoxml(lexicon_data, attr_type=False)\n",
    "xml2 = xml2.decode(\"utf-8\")\n",
    "s = re.sub(r'&gt;', r'>', xml2)\n",
    "s = re.sub(r'&lt;', r'<', s)\n",
    "s = re.sub(r'&quot;', r'\"', s)\n",
    "with open(\"/home/stockhausen/Dokumente/projekte/pta_lexika/pta_lexicon_grc.xml\", 'w', encoding='utf-8-sig') as file_open:\n",
    "    file_open.write(s)\n",
    "# anschlieÃŸend: xmllint --format pta_lexicon_grc.xml -o pta_lexicon_grc.xml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
